<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html">

<head>
  <meta charset="utf-8">
  <title>Bias in Personalized Rankings: Concepts to Code</title>
  <meta content="width=device-width, initial-scale=1.0" name="viewport">
  <meta content="" name="keywords">
  <meta content="" name="description">

  <!-- Favicons -->
  <link href="img/favicon.png" rel="icon">
  <link href="img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,700,700i|Raleway:300,400,500,700,800" rel="stylesheet">

  <!-- Bootstrap CSS File -->
  <link href="lib/bootstrap/css/bootstrap.min.css" rel="stylesheet">

  <!-- Libraries CSS Files -->
  <link href="lib/font-awesome/css/font-awesome.min.css" rel="stylesheet">
  <link href="lib/animate/animate.min.css" rel="stylesheet">
  <link href="lib/venobox/venobox.css" rel="stylesheet">
  <link href="lib/owlcarousel/assets/owl.carousel.min.css" rel="stylesheet">

  <!-- Main Stylesheet File -->
  <link href="css/style.css" rel="stylesheet">

</head>

<body>

  <!--==========================
    Header
  ============================-->
  <header id="header">
    <div class="container">

      <div id="logo" class="pull-left">
        <h1><a href="#intro-icdm">BiasPersRanking</a></h1>
      </div>

      <nav id="nav-menu-container">
        <ul class="nav-menu">
          <li><a href="#abstract">Introduction</a></li>
          <li><a href="#scope">Rationale</a></li>
          <li><a href="#audience">Target Audience</a></li>
          <li><a href="#structure">Outline</a></li>
          <li><a href="#resources">Material</a></li>
          <li><a href="#organizers">Presenters</a></li>
          <li><a href="#contacts">Contacts</a></li>
          <li><a href="#editions">Past Editions</a></li>
        </ul>
      </nav><!-- #nav-menu-container -->
    </div>
  </header><!-- #header -->

  <!--==========================
    Intro Section
  ============================-->
  <section id="intro">
    <div class="intro-container wow fadeIn">
      <h1 class="mb-4 pb-0">Tutorial on<br/> Bias in Personalized Rankings: Concepts to Code</h1>
      <p class="mb-4 pb-0">to be held as part of the <u><a href="http://icdm2020.bigke.org/" target="_blank">20th IEEE International Conference on Data Mining (ICDM2020)</a></u></p>
      <p class="mb-4 pb-0">November, 2020 - ONLINE </p>
    </div>
  </section>

  <main id="main">

    <!--==========================
    Introduction Section
    ============================-->
    <section id="abstract" class="wow fadeInUp">

      <div class="container-fluid">
        <div class="section-header">
          <h2>Introduction</h2>
          <p>This tutorial aims to provide the ICDM community with a comprehensive survey on <strong>data and algorithmic bias</strong> in <strong>personalized rankings</strong>, with a focus on state-of-the-art recommendation algorithms and systems. First, we present conceptual fundamentals on bias based on the existing literature, including a broad discussion on real-world perspectives, ethical aspects, and system's objectives impacted by this phenomenon. Then, we dive deeper into concepts, techniques, metrics, and frameworks that allow to <strong>detect</strong>, <strong>understand</strong>, and <strong>mitigate bias</strong> along the algorithm design pipeline, by presenting the theoretical foundations behind state-of-the-art approaches and their implementations. We showcase <strong>hands-on examples</strong> of pre-, in-, and post-processing bias mitigation techniques by means of open-source tools and public datasets, engaging tutorial participants in mitigation design and in articulating <strong>impacts on stakeholders</strong>. Finally, we shed light on how the research in this novel and fast-growing field has opened challenges and opportunities for the ICDM community.</p>
        </div>
      </div>

    </section>

    <!--==========================
    Rationale Section
    ============================-->
    <section id="scope" class="wow fadeInUp section-with-bg" style="padding: 60px 0 30px 0;">

      <div class="container-fluid">
        <div class="section-header">
          <h2 style="margin: 10px auto 30px auto;">Rationale</h2>

          <p>Big Data, Artificial Intelligence, and High-Processing Computing are transforming our society more and more. <strong>Intelligent systems</strong> are becoming pervasive in our daily lives and shaping both our online and onlife experience, from search to recommendations. Over the last few years, <strong>investigating the potential consequences</strong> of the (semi-)automatic decision-making support provided by intelligent systems has become a topic of primary relevance. For instance, Amazon ended up to scrap its recruiting system, because it took sexist decisions. Hence, given the increasing adoption of systems empowered with AI capabilities, it is crucial to <strong>ensure that their decisions do not lead to biased or even discriminatory outcomes</strong>.</p>

          <p>Over the technologies getting attention in recent years, <strong>ranking and recommender systems</strong> are playing a key role in today's online platforms, finally influencing how and what information individuals access. However, they can create <strong>biases</strong> which may lead to treatment disparity and filter bubbles for information seekers. These systems are trained on historical data, which often conveys <strong>imbalances</strong> and <strong>inequalities</strong>. Such patterns might be captured and emphasized in the results these algorithms provide to users, creating <strong>position and exposure biases</strong> and often providing <strong>unfair results</strong>. This happens when algorithms systematically discriminate users as individuals or as belonging to a legally-protected class.</p>

          <p>Given that bias is becoming a threat to information retrieval, the capability to <strong>uncover</strong>, <strong>characterize</strong>, and <strong>counteract biases</strong>, while preserving the effectiveness of ranking and recommendation systems, is proving to be prominent and timely. Controlling the effects generated by popularity bias on the quality of the results, supporting consumers and providers with fair recommendations, and providing transparent results are examples of current challenges. Even though biases and their consequences are receiving more and more attention by the machine learning community, the <strong>complex ecosystems</strong> wherein recommender systems stand and the <strong>specificity of the modern approaches</strong> are requiring solutions tailored for the recommendation task.</p>

          <p>Bias in personalized rankings is being recognized as a <strong>critical problem</strong> in current AI research and development. At the core of the problem is to study the design space of bias issues, gaining insights on how these systems work and why they may fail to deliver less biased and unfair results. The research on this topic should at least include <strong>three key components</strong>:</p>

          <ul>
              <li>study the interdisciplinary concepts and problem space;</li>
              <li>formulate and design a bias-aware algorithmic pipeline;</li>
              <li>materialize, understand, and mitigate the effects of bias. </li>
          </ul>

          <p>The proposed tutorial will allow the ICDM community to delve into this area, with concrete and practical expertise. Specifically, our tutorial will cover the <strong>foundations of algorithmic bias</strong> of and how it can be assessed and mitigated in ranking and recommender systems, by presenting an overview of the existing literature. Since our <strong>hands-on use cases</strong> will be focused on state-of-the-art recommenders and we assume not all the audience interested in societal issues might be familiar with this class of algorithms, we start the second session with a brief overview on <strong>modern recommendation techniques</strong> based on collaborative filtering, with pointers to relevant literature and with an introductory notebook presenting possible implementations. We will continue our tutorial by putting the theory we presented in the first slot into practice, and present <strong>how to assess and mitigate bias</strong> in item recommendation. Specifically, we will consider two use-case scenarios in which bias (i) comes from data distribution <strong>popularity bias</strong> and (ii) is associated to protected attributes of item providers <strong>provider fairness</strong>.</p>

          <p>This tutorial provides attendees with a <strong>novel perspective</strong> of inspecting personalized rankings, enabling them to consider beyond-accuracy perspectives, in state-of-the-art collaborative filtering algorithms. The algorithmic approaches presented would help not only academic researchers but also industrial practitioners to better develop systems that <strong>tackle bias constraints</strong>.</p>

        </div>
      </div>

    </section>

    <!--==========================
    Target Audience Section
    ============================-->
	
    <section id="audience" class="wow fadeInUp" style="padding: 60px 0 30px 0;">

      <div class="container-fluid">
        <div class="section-header">
          <h2>Target Audience</h2>

          <p>This tutorial is accessible to <strong>researchers</strong>, <strong>industry technologists</strong> and <strong>practitioners</strong>. For people not familiar with rankings, this tutorial covers necessary <strong>background material</strong>. No prior knowledge on bias is assumed.</p>

          <p>One aspect relevant from the outline is that bias is a <strong>highly interdisciplinary topic</strong>, touching on several dimensions beyond algorithms. Hence, our tutorial is of interest for an interdisciplinary audience attending ICDM, with <strong>different backgrounds</strong>.</p>

          <p>After this tutorial, <strong>attendees will be able to</strong> understand key aspects of bias in personalized rankings, materialize biases into underlying systems, play with mitigation and articulate impacts on stakeholders, identify challenges and opportunities.</p>

        </div>
      </div>

    </section>

    <!--==========================
    Outline Section
    ============================-->
    <section id="structure" class="wow fadeInUp">

      <div class="container-fluid">
        <div class="section-header">
          <h2>Outline</h2>

          <p>The ongoing worldwide COVID-19 situation pushed ICDM 2020 organizers to make the conference completely virtual.</p>

		  <div style="margin: 10px auto 30px auto; width: 50%;">
			  <table class="table">
				  <thead>
					<tr>
					  <th scope="col" class="time">Timing</th>
					  <th scope="col">Content</th>
					</tr>
				  </thead>
				  <tbody>
					<tr>
					  <th scope="row" class="time"><strong>5 mins</strong></th>
					  <td><strong>Welcome and Presenters' Introduction</strong></td>
					</tr>
					<tr>
					  <th scope="row" class="time"><strong>150 mins</strong></th>
					  <td><strong>Session I: Foundations</strong></td>
					</tr>
					<tr>
					  <th scope="row" class="time">10 mins</th>
					  <td>
                          Recommendation Principles
                          <ul>
                              <li><strong>Recommendation principles</strong>. To introduce the problems associated to algorithmic bias, we will present the recommendation task as the  generation of the most effective personalized ranking for a user, as in modern recommender systems.</li>
                              <li><strong>Multi-sided recommendation aspects</strong>. Recommender systems have an impact on multiple actors, namely consumers, providers, system's owners. We will present these actors and the phases of the recommendation process where they  have a role (design, algorithm, and evaluation).</li>
                          </ul>
                      </td>
					</tr>
					<tr>
					  <th scope="row" class="time">30 mins</th>
					  <td>
                          Algorithmic Bias Foundations
                          <ul>
                              <li><strong>Motivating examples</strong>. We will present real-world examples where bias can impact recommendation, considering  domains such as music, education, social platforms, and recruiting.</li>
                              <li><strong>Perspectives impacted by bias</strong>. Bias has an impact on several perspectives such as the economy, law, society, security, technology, and psychology.</li>
                              <li><strong>Ethical aspects influenced by bias</strong>. Bias can have an impact at the ethical level and lead to issues such as recommendation of inappropriate content, lack of privacy, violation of autonomy and identity, introduction of opacity, lack of fairness, or the compromising of users' social relationships.</li>
                              <li><strong>Objectives influenced by bias</strong>. We will present recommendation objectives influenced by bias (utility, coverage, diversity, novelty, visibility, exposure) and provide examples of related work.</li>
                          </ul>
                      </td>
					</tr>
					<tr>
					  <th scope="row" class="time">25 mins</th>
					  <td>
                          Bias through the Pipeline
                          <ul>
                              <li><strong>Recommendation pipeline</strong>. We will provide an initial overview of the recommendation pipeline, to characterize how bias can exist at several stages, namely, data acquisition and storage, data preparation, model training, model prediction, model evaluation, and recommendation delivery.</li>
                              <li><strong>Types of bias associated to the pipeline</strong>. We explore the types of bias that can emerge at different stages of the pipeline, i.e., those associated to the users,  platforms, data collection, data preparation, model exploitation, and model evaluation.</li>
                          </ul>
                      </td>
					</tr>
					<tr>
					  <th scope="row" class="time">30 mins</th>
					  <td>
                          Discrimination
                          <ul>
                              <li><strong>Types of discrimination</strong>. When bias affects users' sensitive attributes, it may lead to discrimination. We present concepts, such as direct/indirect discrimination, its granularity (group, individual, and subgroup discrimination), types of disparity (disparate treatment, impact, and mistreatment).</li>
                              <li><strong>Definitions of fairness</strong>. We will present definitions of fairness and different classes in which thy can be categorized (equalized odds, equalized opportunity, demographic parity, fairness through (un)awareness, equality of treatment).</li>
                          </ul>
                      </td>
					</tr>
					<tr>
					  <th scope="row" class="time">40 mins</th>
					  <td>
                          Mitigation Design
                          <ul>
                              <li><strong>Bias-aware process pipeline</strong>. Intervention strategies to mitigate algorithmic bias require an analysis of where and how bias might affect the system. We present a pipeline to support mitigation design.</li>
                              <li><strong>Techniques for bias treatment</strong>. We will present the three main classes of mitigation techniques (pre-, in-, and post-processing), along with examples of solutions proposed for recommender systems.</li>
                              <li><strong>Real-world applications</strong>. We will present examples of real-world platforms, such as LinkedIn and Spotify, and of their approaches to deal with bias..</li>
                          </ul>
                      </td>
					</tr>
					<tr>
					  <th scope="row" class="time"><strong>10 mins</strong></th>
					  <td><strong>Questions and Discussion</strong></td>
					</tr>
					<tr>
					  <th scope="row" class="time"><strong></strong></th>
					  <td><strong>Coffee Break</strong></td>
					</tr>
					<tr>
					  <th scope="row" class="time"><strong>150 mins</strong></th>
					  <td><strong>Session II: Hands-on Studies</strong></td>
					</tr>
					<tr>
					  <th scope="row" class="time">30 mins</th>
					  <td>Recommender Systems in Practice</td>
                      <ul>
                          <li>Data preparation starting from public datasets (i.e., COCO and Movielens datasets).</li>
                          <li>Model definition (e.g., user/item embeddings, layers stacking) and  training (e.g., epochs, loss, optimizer, learning rate)</li>
                          <li>User-item relevance matrix computation from a pre-trained model (e.g., model load, predictions).</li>
                          <li>Model evaluation oriented to utility (e.g., NDCG, beyond-accuracy metrics).</li>
                      </ul>
					</tr>
					<tr>
					  <th scope="row" class="time"><strong>45 mins</strong></th>
					  <td>
                          Investigation on Item Popularity Bias
                          <ul>
                              <li>Definition and characterization of item popularity biases in interactions and recommendations.</li>
                              <li>Application of mitigation techniques based on pre-, in-, and post-processing.</li>
                              <li>Comparison of mitigation techniques based on bias and recommendation utility trade-offs.</li>
                              <li>Comparison of mitigation techniques on beyond-utility metrics (e.g., coverage, diversity, novelty).</li>
                          </ul>
                      </td>
					</tr>
					<tr>
					  <th scope="row" class="time"><strong>45 mins</strong></th>
					  <td>
                          Investigation on Item Provider Fairness
                          <ul>
                              <li>Association of items to providers and sensitive attributes and characterization of providers representation in the catalog and in the interactions..</li>
                              <li>Identification of minority providers, both at individual and group level.</li>
                              <li>Definition and measurement of item provider unfairness on recommendations.</li>
                              <li>Application of mitigation techniques based on pre-, in-, and post-processing.</li>
                              <li>Comparison of mitigation techniques based on fairness and recommendation utility trade-offs.</li>
                              <li>Comparison of mitigation techniques on beyond-utility metrics (e.g., coverage, diversity, novelty).</li>
                          </ul>
                      </td>
					</tr>
					<tr>
					  <th scope="row" class="time"><strong>10 mins</strong></th>
					  <td><strong>Open Issues and Research Challenges</strong></td>
					</tr>
					<tr>
					  <th scope="row" class="time"><strong>20 mins</strong></th>
					  <td><strong>Questions and Discussion</strong></td>
					</tr>
				  </tbody>
			  </table>
		  </div>
        </div>
      </div>

    </section>

    <!--==========================
    Material Section
    ============================-->
    <section id="resources" class="wow fadeInUp section-with-bg">

      <div class="container-fluid">
        <div class="section-header">
          <h2>Material</h2>

          <p>Lecture slides, Github repository, and Jupyter notebooks will be made available to attendees before the tutorial.</p>

        </div>
      </div>

    </section>

    <!--==========================
    Presenters Section
    ============================-->
    <section id="organizers" class="wow fadeInUp">

      <div class="container-fluid">
        <div class="section-header">
          <h2>Presenters</h2>

          <p style="text-align: center;"><img src="https://media-exp1.licdn.com/dms/image/C4E03AQH12B2k5FNOPg/profile-displayphoto-shrink_200_200/0?e=1603929600&v=beta&t=5l4MVlmpryCUDQhQ3Gs2mmfBpXjpuHvZST9l4cKrO74" class="img-rounded" alt="Ludovico Boratto" style="width:200px;"></p>
          <p style="text-align: center;"><strong><a href="https://www.ludovicoboratto.com/" target="_blank">Dr. Ludovico Boratto</a> </br>EURECAT - Centre Tecn&ograve;logic de Catalunya (Spain)</strong></p>
		  <p>Dr. Ludovico Boratto is senior research scientist in at EURECAT. His research focuses on recommender systems and on their impact on stakeholders. His research has been published in top-tier conferences and journals. He is editor of the book "Group Recommender Systems: An Introduction" (Springer). He is editorial board member of the "Information Processing \& Management" journal (Elsevier) and guest editor of other special issues. He is regularly PC member of the main Data Mining conferences. In 2012, he got a Ph.D. at the University of Cagliari, where he was research assistant until May 2016.</p>
          <br/><br/>
          <p style="text-align: center;"><img src="https://media-exp1.licdn.com/dms/image/C4D03AQET4nEbq071jg/profile-displayphoto-shrink_200_200/0?e=1603929600&v=beta&t=xAR2UKfide1peSOrwnDSjDNYm9hE5x3u5AgU02ilr7E" class="img-rounded" alt="Mirko Marras" style="width:200px;"></p>
          <p style="text-align: center;"><strong><a href="https://www.mirkomarras.com/" target="_blank">Dr. Mirko Marras</a> </br>University of Cagliari (Italy)</strong></p>
		  <p>Dr. Mirko Marras is a Postdoctoral Researcher at the University of Cagliari (Italy). His research focuses on machine learning for recommender systems, with particular attention to bias issues. He authored papers in top-tier journals, such as Pattern Recognition Letters and Computers Human Behavior. He gave talks and demos at international conferences and workshops, e.g., TheWebConf2018, ECIR2019, and INTERSPEECH2019. He is PC member of major conferences, e.g., ACL, AIED, EDM, ECML-PKDD, EMNLP, ITICSE, ICALT, UMAP. He co-chaired the BIAS2020 workshop at ECIR2020 and gave a tutorial on Bias in Recommender Systems at UMAP2020. In 2020, he received a Doctoral Degree from University of Cagliari.</p>

        </div>
      </div>

    </section>

    <!--==========================
    Contacts Section
    ============================-->
    <section id="contacts" class="wow fadeInUp section-with-bg">

      <div class="container-fluid">
        <div class="section-header">
          <h2>Contacts</h2>
          <p>Please, reaching out to us at <strong>ludovico.boratto@acm.org</strong> and <strong>mirko.marras@unica.it</strong>.</p>
        </div>
      </div>

    </section>

    <!--==========================
    Editions Section
    ============================-->
    <section id="editions" class="wow fadeInUp">

      <div class="container-fluid">
        <div class="section-header">
          <h2>Past Editions</h2>
          <p>We also invite you to check out previous editions of our similar tutorials:</p>
          <ul>
              <li><a href="https://mirkomarras.github.io/bias-recsys-tutorial/umap2020" target="_blank">Hands-on Tutorial on Data and Algorithmic Bias in Recommender Systems</a></li>
          </ul>
        </div>
      </div>

    </section>


  </main>

  <a href="#" class="back-to-top"><i class="fa fa-angle-up"></i></a>

  <!-- JavaScript Libraries -->
  <script src="lib/jquery/jquery.min.js"></script>
  <script src="lib/jquery/jquery-migrate.min.js"></script>
  <script src="lib/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="lib/easing/easing.min.js"></script>
  <script src="lib/superfish/hoverIntent.js"></script>
  <script src="lib/superfish/superfish.min.js"></script>
  <script src="lib/wow/wow.min.js"></script>
  <script src="lib/venobox/venobox.min.js"></script>
  <script src="lib/owlcarousel/owl.carousel.min.js"></script>

  <!-- Contact Form JavaScript File -->
  <script src="contactform/contactform.js"></script>

  <!-- Template Main Javascript File -->
  <script src="js/main.js"></script>
</body>

</html>
